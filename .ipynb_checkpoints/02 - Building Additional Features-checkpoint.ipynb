{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d8ecaff-c6d0-4571-9c25-db36c8b85cdc",
   "metadata": {},
   "source": [
    "# 02 - Building various feature types in Tecton\n",
    "\n",
    "Tecton provides a lot of flexibility to easily develop and deploy a variety of features. Features may vary based on:\n",
    "- Feature data freshness requirement and availability of input data (Batch, Near real-time, Real-time)\n",
    "- Transformation logic (aggregations, joining, filtering, geographical etc.)\n",
    "- Coding language of choice (Python, Snowflake SQL, PySpark, SparkSQL etc...)\n",
    "\n",
    "In the second part of this workshop, we will explore different types of features to enrich our fraud detection model with additional features and hopefully improve is performance.\n",
    "\n",
    "1. Number of transactions on card in last minute 1/3/5 mins (Streaming)\n",
    "2. Distance between current user location and known Merchant address (Batch + On-demand)\n",
    "3. Merchant historical fraud rate (Batch)\n",
    "4. User account age (Custom Batch ETL)\n",
    "5. Bonus: Transaction country based on lat/long using 3rd party API (On-Demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c94acd2-5372-45f7-8ea5-313b09bb7ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tecton\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "tecton.set_validation_mode(\"auto\")\n",
    "\n",
    "tecton.login('https://demo-nebula.tecton.ai/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8029a81-5903-4855-8782-99f02faa628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import snowflake.connector\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "connection_parameters = {\n",
    "     \"user\": \"YOUR_USER\",\n",
    "    \"password\": \"YOUR_PASSWORD\",\n",
    "    \"account\": \"tectonpartner\",\n",
    "    \"warehouse\": \"NAB_WH\",\n",
    "    # Database and schema are required to create various temporary objects by tecton\n",
    "    \"database\": \"NAB_MFT_DB\",\n",
    "    \"schema\": \"PUBLIC\",\n",
    "}\n",
    "conn = snowflake.connector.connect(**connection_parameters)\n",
    "tecton.snowflake_context.set_connection(conn) # Tecton will use this Snowflake connection for all interactive queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb1f6e9-93b6-43dd-838e-06f4236953eb",
   "metadata": {},
   "source": [
    "## 0. Fetching objects that have been registered to Tecton \n",
    "\n",
    "When developing features with Tecton, we can use Tecton objects that have been previously registered to a workspace and combine them with local Tecton objects. This is particularly helpful when testing new features for a model, you can extend an existing Feature Service with new local features and train a new model version to validate the predictive power of your local features.\n",
    "\n",
    "Here, we will retrieve the ```user``` entity and the ```transactions``` data source we pushed to Tecton in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06124b32-399b-4e52-811b-f3f2c3e09ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = tecton.get_workspace('your-workspace-name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f335fc-e597-4cf9-9f6a-7f6d9d9d877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = ws.get_entity('user')\n",
    "transactions_batch = ws.get_data_source('transactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416a1d21-c720-408d-af65-e11c7d646829",
   "metadata": {},
   "source": [
    "## 1. Number of transactions on card in last minute 1/3/5 mins\n",
    "\n",
    "This feature computes the count of transactions on a card in the last 1,3,5 minutes prior to the current transactions. In most cases, this data typically lands in your data warehouse in a batch way on a regular schedule (e.g 1 Day or 1 Hour). If you were to build features off of this, you would not get the appropriate feature freshness for these features to be meaningful because of the data delay (time it takes for your transaction data to be available in your data source).\n",
    "\n",
    "For features that require very high freshness, we will need to build them off of streaming events.\n",
    "\n",
    "#### ðŸ’¡Streaming features in Tecton\n",
    "Real-time data can make all the difference for real-time models, but leveraging it can be quite the challenge.\n",
    "\n",
    "With Tecton you can build millisecond fresh features using plain Python and without any complex streaming infrastructure! Best of all, you can test it all locally and iterate in a notebook to quickly train better models that operate consistently online and offline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46d5906-ac5d-446f-b5c0-e074fe67756f",
   "metadata": {},
   "source": [
    "### A) Defining our PushSource\n",
    "\n",
    "First, let's define a local Stream Source that supports ingesting real-time data. Once productionized, this will give us an online HTTP endpoint to push events to in real-time which Tecton will then transform into features for online inference.\n",
    "\n",
    "As part of our Stream Source, we also register a historical log of the stream via the batch_config parameter. Tecton uses this historical log for backfills and offline development.\n",
    "\n",
    "ðŸ’¡ Alternatively, you can have Tecton maintain this historical log for you! Simply add the log_offline=True parameter to the PushConfig and omit the batch_config. With this setup Tecton will log all ingested events and use those to backfill any features that use this source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e487e90-cf0b-491c-ba44-7d4f4ba059e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton import StreamSource, PushConfig, SnowflakeConfig\n",
    "from tecton.types import Field, String, Timestamp, Float64\n",
    "\n",
    "offline_config = SnowflakeConfig(\n",
    "    url=\"https://tectonpartner.snowflakecomputing.com/\",\n",
    "    database=\"NAB_MFT_DB\",\n",
    "    schema=\"PUBLIC\",\n",
    "    warehouse=\"NAB_WH\",\n",
    "    table=\"TRANSACTIONS\",\n",
    "    timestamp_field=\"TIMESTAMP\"\n",
    ")\n",
    "\n",
    "transactions = StreamSource(\n",
    "    name=\"transactions_stream\",\n",
    "    stream_config=PushConfig(),\n",
    "    batch_config=offline_config,\n",
    "    schema=[\n",
    "        Field(\"USER_ID\", String), \n",
    "        Field(\"TIMESTAMP\", Timestamp), \n",
    "        Field(\"AMT\", Float64)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac371daf-1d73-4bd6-8046-57f0dd25225f",
   "metadata": {},
   "source": [
    "#### Testing the new source\n",
    "We can pull a range of offline data from a Stream Source's historical event log using ```get_dataframe()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd7018e-7c02-4aac-9257-57d5e0774917",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(2023, 5, 1)\n",
    "end = datetime(2023, 8, 1)\n",
    "\n",
    "df = transactions.get_dataframe(start, end).to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a64d97-a3ac-436e-8076-de3d3da5d30f",
   "metadata": {},
   "source": [
    "### B) Defining and testing Streaming features locally\n",
    "\n",
    "Now that we have a Stream Source defined, we are ready to create some features.\n",
    "\n",
    "Let's use this data source to create the following features:\n",
    "\n",
    "- A user's count of transactions in the last 1/3/5 minutes\n",
    "- A user's total transaction amount in the last 1 hour\n",
    "\n",
    "To build these features, we will define a Stream Feature View that consumes from our transactions Stream Source.\n",
    "\n",
    "The Stream Feature View transformation operates on events in a Pandas Dataframe and can do any arbitrary projections, filters, or expressions as needed. It's just Python!\n",
    "\n",
    "As always, we can combine these transformations with Tecton Aggregations to easily compute common aggregates over time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719fce19-4d27-4739-8f34-01e68762a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton import Entity, stream_feature_view, Aggregation\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "@stream_feature_view(\n",
    "    source=transactions,\n",
    "    entities=[user], #The user entity is reused from our Tecton workspace\n",
    "    mode=\"pandas\",\n",
    "    schema=[Field(\"USER_ID\", String), Field(\"TIMESTAMP\", Timestamp), Field(\"AMT\", Float64)],\n",
    "    aggregations=[\n",
    "        Aggregation(function=\"count\", column=\"AMT\", time_window=timedelta(minutes=1)),\n",
    "        Aggregation(function=\"count\", column=\"AMT\", time_window=timedelta(minutes=3)),\n",
    "        Aggregation(function=\"count\", column=\"AMT\", time_window=timedelta(minutes=5)),\n",
    "        Aggregation(function=\"sum\", column=\"AMT\", time_window=timedelta(hours=1))\n",
    "    ],\n",
    ")\n",
    "def user_transaction_amount_totals(transactions):\n",
    "    return transactions[[\"USER_ID\", \"TIMESTAMP\", \"AMT\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928b988b-9aea-414a-b10c-dca6133e6421",
   "metadata": {},
   "source": [
    "Now that we've defined and validated our Feature View, we can use get_historical_features to produce a range of feature values and check out the feature data.\n",
    "\n",
    "ðŸ’¡**INFO**\n",
    "These features are calculated against the Stream Source's historical event log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0bba98-3d8e-4897-be23-20f81cee2626",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(2022, 1, 1)\n",
    "end = datetime(2022, 2, 1)\n",
    "\n",
    "df = user_transaction_amount_totals.get_historical_features(start_time=start, end_time=end).to_pandas()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612bd656-0356-4d3f-80fd-e4e32fae441e",
   "metadata": {},
   "source": [
    "## 2. Distance between current user location and their home location\n",
    "\n",
    "For this feature, we need to combine data from 2 different sources:\n",
    "- A Batch data source which contains data about our users and their home location (lat/long)\n",
    "- A real-time data source (RequestSource in Tecton terms) which contains the location data for the current transaction that is being scored. This data is passed to Tecton as additional context when querying a Feature Service.\n",
    "\n",
    "In order to create this feature, we will then create 2 Feature Views:\n",
    "- A Batch Feature View that will compute the user's latitude and longitude (refreshed daily)\n",
    "- An On-demand Feature View that will combine the user's latitute and longitude with the current transaction latitude and longitude and return the distance between the two\n",
    "\n",
    "#### A) User's latitude and longitude Feature View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3be37-841f-43e9-bcbd-d6692e60b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton import BatchSource\n",
    "\n",
    "fraud_users = BatchSource(\n",
    "    name='users_batch',\n",
    "    batch_config=SnowflakeConfig(\n",
    "        url=\"https://tectonpartner.snowflakecomputing.com/\",\n",
    "        database=\"NAB_MFT_DB\",\n",
    "        schema=\"PUBLIC\",\n",
    "        warehouse=\"NAB_WH\",\n",
    "        table=\"USERS\",\n",
    "        timestamp_field=\"SIGNUP_TIMESTAMP\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0058481-35ec-417d-928f-32cfe39282b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_users.get_dataframe().to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d699557f-2d7c-4c8d-b9eb-f14458204c09",
   "metadata": {},
   "source": [
    "Now that our data source is defined, we can define a Batch Feature View on top of it. This time, we will use Snowflake SQL mode to define our transformation logic. In this case there is no transformation, we are just doing a simple select from our Snowflake table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fe26cd-18c4-4325-a6e8-91ce91653268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton import batch_feature_view\n",
    "from tecton.types import String, Timestamp, Float64\n",
    "\n",
    "@batch_feature_view(\n",
    "    entities=[user],\n",
    "    sources=[fraud_users],\n",
    "    mode=\"snowflake_sql\",\n",
    "    batch_schedule=timedelta(days=1),\n",
    "    schema=[\n",
    "        Field(\"USER_ID\", String), \n",
    "        Field(\"SIGNUP_TIMESTAMP\", Timestamp), \n",
    "        Field(\"LAT\", Float64), \n",
    "        Field(\"LONG\", Float64)\n",
    "           ]\n",
    ")\n",
    "def user_home_location(fraud_users):\n",
    "    return f\"\"\"\n",
    "    select\n",
    "        USER_ID,\n",
    "        SIGNUP_TIMESTAMP,\n",
    "        LAT,\n",
    "        LONG\n",
    "    from {fraud_users}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119ee266-2747-4503-84e3-bc1c4ebec4e2",
   "metadata": {},
   "source": [
    "Now we can test our feature view using ```.get_historical_features()``` on a given time range. Here Tecton will compute all feature values between 2017-01-01 and 2023-11-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff3a2ab-ad71-46aa-a4f1-d372e9596e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(2017, 1, 1)\n",
    "end = datetime(2023, 11, 1)\n",
    "\n",
    "df = user_home_location.get_historical_features(start_time=start, end_time=end).to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4126df69-d5eb-4225-be37-cf2a8b620a3f",
   "metadata": {},
   "source": [
    "#### B) Distance between current transaction location and user's home location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb0d9d6-8978-4191-be26-767886dc5f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton import on_demand_feature_view, RequestSource\n",
    "\n",
    "request_source = RequestSource(\n",
    "    schema=[Field('MERCH_LAT', Float64),Field('MERCH_LONG', Float64)]\n",
    ")\n",
    "\n",
    "@on_demand_feature_view(\n",
    "    sources=[\n",
    "        request_source,\n",
    "        user_home_location\n",
    "    ],\n",
    "    mode='python',\n",
    "    schema=[Field('dist_km', Float64)],\n",
    "    description=\"How far a transaction is from the user's home\",\n",
    ")\n",
    "def user_to_transaction_distance(request_source, user_home_location):\n",
    "    from math import sin, cos, sqrt, atan2, radians\n",
    "\n",
    "    # Approximate radius of earth in km\n",
    "    R = 6373.0\n",
    "\n",
    "    lat1 = radians(request_source['MERCH_LAT'])\n",
    "    lon1 = radians(request_source['MERCH_LONG'])\n",
    "    lat2 = radians(user_home_location['LAT'])\n",
    "    lon2 = radians(user_home_location['LONG'])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "\n",
    "    return {'dist_km': distance}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ac72a7-7a50-4b60-9191-16a83c0006cd",
   "metadata": {},
   "source": [
    "#### Testing our On-demand feature view against a set of historical transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d58e39e-e8f5-4d60-b25d-d6c413cd1db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_snowflake(query):\n",
    "    df = conn.cursor().execute(query).fetch_pandas_all()\n",
    "    return df\n",
    "\n",
    "training_events = query_snowflake(\"\"\"\n",
    "    select \n",
    "        USER_ID,\n",
    "        TIMESTAMP,\n",
    "        AMT,\n",
    "        IS_FRAUD,\n",
    "        MERCH_LAT,\n",
    "        MERCH_LONG\n",
    "    from\n",
    "        TRANSACTIONS\n",
    "    limit 100\n",
    "\"\"\")\n",
    "\n",
    "user_to_transaction_distance.get_historical_features(training_events).to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f08fda-4763-49de-bc78-b90ad4f80dcf",
   "metadata": {},
   "source": [
    "## 3. Merchant Fraud rate in the past year (Batch Feature View + Aggregations)\n",
    "\n",
    "This one is an example of a simple window aggregation feature. Refer to the example from notebook 1 and fill the gaps in the cell below. \n",
    "\n",
    "**Hint:** you will need to create the merchant entity. The IS_FRAUD column is a 0,1 flag that indicates whether a transaction was labelled as fraudulent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4823a5-e963-4ba4-b58a-200b94866369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton import batch_feature_view, Aggregation\n",
    "\n",
    "merchant = Entity(\n",
    "    name=\"merchant\",\n",
    "    join_keys=[], ### FILL THE ENTITY JOIN KEY HERE\n",
    "    description=\"A merchant that receives transactions\"\n",
    ")\n",
    "\n",
    "@batch_feature_view(\n",
    "    entities=[merchant],\n",
    "    sources=[transactions_batch],\n",
    "    mode=\"pandas\",\n",
    "    aggregation_interval=timedelta(days=1),\n",
    "    aggregations=[], ### FILL THE AGGREGATION FUNCTION HERE\n",
    "    \n",
    "    schema=[Field(\"MERCHANT\", String), Field(\"TIMESTAMP\", Timestamp), Field(\"IS_FRAUD\", Int64)]\n",
    ")\n",
    "def merchant_fraud_rate(transactions):\n",
    "    \"\"\"\n",
    "    INSERT FEATURE VIEW LOGIC HERE\n",
    "    \n",
    "    \"\"\"\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5390aac-8451-427d-acea-e09125ad5215",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(2023, 11, 1)\n",
    "end = datetime(2023, 11, 2)\n",
    "\n",
    "df = merchant_fraud_rate.get_historical_features(start_time=start, end_time=end).to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e66cc2-3960-43d0-bbb0-dcea0accd185",
   "metadata": {},
   "source": [
    "## 4. User account age (Custom ETL feature)\n",
    "\n",
    "If you need to define aggregation features that aren't supported by Tecton's Aggregation Engine, you can develop [custom ETL features](https://docs.tecton.ai/docs/defining-features/feature-views/batch-feature-view/simplifying-custom-batch-aggregations-with-incremental-backfills).\n",
    "\n",
    "When using custom ETL features, disable Tectonâ€™s optimized backfill by setting ```incremental_backfills``` to ```True```. Here's why you need to disable it, and how it works:\n",
    "\n",
    "- Default behavior: Tecton runs your query once for the entire time between feature_start_time and the feature registration time, minimizing backfill jobs (as explained here). This isn't compatible with ETL features that expect to process a fixed time range (say 1 day worth of data) of data for every single query run.\n",
    "- Incremental Backfills: Your query runs separately for each batch_schedule interval in the specified time range, ensuring every query run gets to process a fixed time range of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018fd305-5d80-4eaf-8463-f954df51f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton import materialization_context\n",
    "\n",
    "@batch_feature_view(\n",
    "    entities=[user],\n",
    "    sources=[fraud_users],\n",
    "    mode=\"snowflake_sql\",\n",
    "    batch_schedule=timedelta(days=1),\n",
    "    incremental_backfills=True,\n",
    "    schema=[Field(\"USER_ID\",String), Field(\"TIMESTAMP\", Timestamp), Field(\"DAYS_SINCE_SIGNUP\", Int64)]\n",
    ")\n",
    "def user_days_since_signup(fraud_users, context=materialization_context()):\n",
    "    return f\"\"\"\n",
    "    select \n",
    "        USER_ID,\n",
    "        TO_TIMESTAMP('{context.end_time}') - INTERVAL '1 MICROSECOND' as TIMESTAMP,\n",
    "        DATEDIFF(day, \"SIGNUP_TIMESTAMP\", TO_TIMESTAMP('{context.start_time}')) as DAYS_SINCE_SIGNUP\n",
    "    from \n",
    "        {fraud_users}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97fdb9-51db-4225-a644-840622cd2615",
   "metadata": {},
   "source": [
    "For custom ETL features, we can't produce feature values for a date range spanny multiple materialization periods in a notebook directly. Instead we can use the ```.run()``` function to run our feature transformation code for a single materialization period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0665a7c2-d0ac-471d-8c66-91f7b28aac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "start= pd.to_datetime(\"today\") - timedelta(days=1)\n",
    "end=pd.to_datetime(\"today\")\n",
    "\n",
    "df=user_days_since_signup.run(start_time=start, end_time=end).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc467423-1464-484a-8a19-cee7f70c84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32555b2d-0676-4b49-9572-c88331ac1b6e",
   "metadata": {},
   "source": [
    "## 5. BONUS: Using a 3rd party API as a data source \n",
    "\n",
    "We will use the openstreetmap free API to geocode the merchant latitude and longitude into the country and city of the merchant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc36c9d-3761-49ce-800d-1579d72a23bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton import on_demand_feature_view, RequestSource\n",
    "from tecton.types import Field, Float64, String\n",
    "\n",
    "request_schema = [Field(\"MERCH_LAT\", Float64), Field(\"MERCH_LONG\", Float64)]\n",
    "transaction_request = RequestSource(schema=request_schema)\n",
    "output_schema = [Field(\"city\", String),Field(\"country\", String)]\n",
    "\n",
    "\n",
    "@on_demand_feature_view(\n",
    "    sources=[transaction_request],\n",
    "    name=\"geocoded_address\",\n",
    "    mode=\"python\",\n",
    "    schema=output_schema,\n",
    "    #environments=[\"tecton-python-extended:0.3\"],\n",
    "    owner=\"vince@tecton.ai\",\n",
    ")\n",
    "def geocoded_city(transaction_request):\n",
    "  import requests, json\n",
    "  headers = {\n",
    "    'User-Agent': 'My User Agent 1.0',\n",
    "    'From': 'vince@tecton.ai'  # This is another valid field\n",
    "  }\n",
    "  lat = transaction_request.get('MERCH_LAT')\n",
    "  lon = transaction_request.get('MERCH_LONG')\n",
    "  url = 'https://nominatim.openstreetmap.org/reverse'\n",
    "  params={\n",
    "    'format':'jsonv2',\n",
    "    'lon':str(lon),\n",
    "    'lat':str(lat)\n",
    "  }\n",
    "  response = requests.get(url, params=params, headers=headers)\n",
    "  if response.status_code==200:\n",
    "    r = json.loads(response.text)\n",
    "    address=r.get('address',{})\n",
    "    return {\n",
    "      'country':address.get('country'), \n",
    "      'city':address.get('city')\n",
    "      }\n",
    "  else:\n",
    "    return {'country':None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1201242-43a3-4ebf-9748-63b289d0145b",
   "metadata": {},
   "source": [
    "#### We can test this On-Demand Feature View with mock inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463d1c52-5d50-4fa3-b81d-5b5c679f437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoded_city.run(\n",
    "  transaction_request={'MERCH_LAT':40.730610,'MERCH_LONG':-73.935242}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3bc64d-f352-4277-ab15-15a9d8147e57",
   "metadata": {},
   "source": [
    "## 6. Deploying things out to Tecton \n",
    "\n",
    "Modify the features.py file to add the new Feature View definitions and call ```tecton plan``` then ```tecton apply``` to push these new definitions to your workspace\n",
    "\n",
    "Once these have been deployed, head to the Tecton UI to monitor the materialization jobs.\n",
    "\n",
    "You are now ready to call your new Feature Service!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de336f45-f19f-4d60-8340-71326762a404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
