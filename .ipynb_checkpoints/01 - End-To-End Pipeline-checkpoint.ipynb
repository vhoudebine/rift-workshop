{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dec1e8d3-4a01-47b0-a09c-674d8c04314e",
   "metadata": {},
   "source": [
    "# Lesson 1 - End-To-End Pipeline\n",
    "This lesson will review the workflow of creating a feature view in Tecton, testing it, and pushing it to your Tecton instance when you're done.\n",
    "\n",
    "In lesson you will:\n",
    "* Build an end-to-end feature pipelines using the core Tecton components:\n",
    "  * Data Source\n",
    "  * Entity\n",
    "  * Feature View\n",
    "  * Feature Service\n",
    "* Query a Tecton feature service to extract features for both training and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b79f3b-725b-4f1f-9d29-3ea77611ea12",
   "metadata": {},
   "source": [
    "### ðŸ”‘ Concept: 5 Ways to Interact with Tecton\n",
    "\n",
    "There are 5 ways of interacting with Tecton:\n",
    "\n",
    "1. **The Tecton Web UI:** The Tecton Web UI is where you can browse, discover, and monitor all of the data sources, features, and more that have been registered with the cluster using `tecton apply`. This is where you can discover other features in your organization that may be helpful for you model or check on the materialization statuses of new features.\n",
    "2. **The Tecton SDK with Notebook Driven Development:** Tecton's SDK allows you to rapidly experiment with new features views and data sources within a notebook environment.  Tecton's SDK can be used in any EMR or Databricks notebook (like this one!) to fetch data from the Feature Store. This includes things like previewing feature data, testing transformations, and building training data sets. Currently the SDK requires a Spark Context, but soon we will offer support for using the Tecton SDK from a local notebook without Spark.\n",
    "3. **The Tecton Feature Repo and CLI:** Data sources, features, and feature sets are all defined as python configuration files in a local \"Feature Repo\" typically backed by git (such as the one you cloned earlier). These definitions are then applied to a workspace in a Tecton cluster using the CLI command `tecton apply`. This will be one of the most common CLI commands you will use.\n",
    "4. **The Tecton REST API:** Tecton's REST API is used for fetching the latest feature values in production for model inference. This endpoint typically returns a feature vector in ~5 milliseconds.\n",
    "5. **The Tecton API Client:** Tecton's Python and Java API clients make using the REST API easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac6d230-1a61-4ffb-9a2c-6c4a44c358f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install â€“pre â€˜tecton[rift, snowflake]â€™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c9195d-898e-452b-ae39-d9901e8bb155",
   "metadata": {},
   "source": [
    "# 0. Initialize your session\n",
    "## Logging into Tecton (and Snowflake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8abbfa-c38e-4c8a-a586-7a43243654f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tecton\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "from datetime import datetime, timedelta\n",
    "from pprint import pprint\n",
    "\n",
    "connection_parameters = {\n",
    "    \"user\": \"YOUR_USER\",\n",
    "    \"password\": \"YOUR_PASSWORD\",\n",
    "    \"account\": \"tectonpartner\",\n",
    "    \"warehouse\": \"NAB_WH\",\n",
    "    # Database and schema are required to create various temporary objects by tecton\n",
    "    \"database\": \"NAB_MFT_DB\",\n",
    "    \"schema\": \"PUBLIC\",\n",
    "}\n",
    "conn = snowflake.connector.connect(**connection_parameters)\n",
    "tecton.snowflake_context.set_connection(conn) # Tecton will use this Snowflake connection for all interactive queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d5b79f-ad4f-453f-85b2-c29928261072",
   "metadata": {},
   "outputs": [],
   "source": [
    "tecton.login('https://demo-nebula.tecton.ai/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdefe6c-dbd9-4b83-821c-b2099bc3992a",
   "metadata": {},
   "source": [
    "# 1. Create your first feature pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d6b45-1aca-467c-acf3-8ff651aae14e",
   "metadata": {},
   "source": [
    "## A) Create Data Source\n",
    "\n",
    "Data sources define a connection to a batch, stream, push, or request data source (i.e. request-time parameters) and are used as inputs to feature pipelines, known as \"Feature Views\" in Tecton.\n",
    "\n",
    "\n",
    "You have 3 options when developing a [data source](https://docs.tecton.ai/docs/defining-features/data-sources):\n",
    "\n",
    "  1. Create a data source with dummy data,\n",
    "  2. Create a data source connected to [actual data (s3, Snowflake etc.)](https://docs.tecton.ai/docs/setting-up-tecton/connecting-data-sources/connecting-data), or\n",
    "  3. Reference and reuse a data source that has already been defined and registered to Tecton.\n",
    "\n",
    "For this example, we will create a Snowflake data source to connect to a Snowflake Table, but see sample code for options 1 and 3 below:\n",
    "\n",
    "**- Create a data source with dummy data:**\n",
    "```python\n",
    "from tecton import BatchSource, pandas_batch_config\n",
    "from datetime import datetime \n",
    "\n",
    "sample_data = [{\n",
    "    \"user_id\": \"12345\",\n",
    "    \"timestamp\": datetime(2023, 3, 1),\n",
    "    \"amt\": 100,\n",
    "    \"product\": \"A\"\n",
    "}, {\n",
    "    \"user_id\": \"12345\",\n",
    "    \"timestamp\": datetime(2023, 2, 1, 15),\n",
    "    \"amt\": 200,\n",
    "    \"product\": \"B\"\n",
    "}, {\n",
    "    \"user_id\": \"12345\",\n",
    "    \"timestamp\": datetime(2023, 1, 1),\n",
    "    \"amt\": 300,\n",
    "    \"product\": \"Q\"\n",
    "}, {\n",
    "    \"user_id\": \"54321\",\n",
    "    \"timestamp\": datetime(2023, 3, 1, 15),\n",
    "    \"amt\": 200,\n",
    "    \"product\": \"C\"\n",
    "}, {\n",
    "    \"user_id\": \"54321\",\n",
    "    \"timestamp\":datetime(2023, 2, 1, 12),\n",
    "    \"amt\": 200,\n",
    "    \"product\": \"D\"\n",
    "}]\n",
    "\n",
    "@pandas_batch_config(supports_time_filtering=True)\n",
    "def dummy_function(filter_context):\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    if filter_context:\n",
    "        if filter_context.start_time:\n",
    "            df = df[df[\"timestamp\"] >= filter_context.start_time]\n",
    "        if filter_context.end_time:\n",
    "            df = df[df[\"timestamp\"] < filter_context.end_time]\n",
    "    return df\n",
    "\n",
    "transactions_dummy = BatchSource(\n",
    "    name='transactions_dummy',\n",
    "    batch_config=dummy_function,\n",
    ")\n",
    "```\n",
    "\n",
    "**- Reference an already created data source:**\n",
    "```python\n",
    "ws = tecton.get_current_workspace()\n",
    "transactions_batch = ws.get_data_source(\"transactions_batch\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6152aa-1197-4dc1-bd5e-fd23072d8ef3",
   "metadata": {},
   "source": [
    "#### 1. Create a data source from a Snowflake table\n",
    "Table name: **TECTON_DEMO_DATA.FRAUD_DEMO.TRANSACTIONS_EXT**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9949a6-8a2d-41a1-88ea-118387769c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton import SnowflakeConfig, BatchSource\n",
    "\n",
    "snowflake_config = SnowflakeConfig(\n",
    "    url=\"https://tectonpartner.snowflakecomputing.com/\",\n",
    "    database=\"NAB_MFT_DB\",\n",
    "    schema=\"PUBLIC\",\n",
    "    warehouse=\"NAB_WH\",\n",
    "    table=\"TRANSACTIONS\",\n",
    "    timestamp_field=\"TIMESTAMP\"\n",
    ")\n",
    "\n",
    "transactions = BatchSource(\n",
    "    name='transactions',\n",
    "    batch_config=snowflake_config,\n",
    ")\n",
    "\n",
    "transactions.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a64ef-e612-451d-bff4-118a57d22786",
   "metadata": {},
   "source": [
    "#### 2. Inspect data source using .get_dataframe()\n",
    "Once a Tecton data source is defined, you can read data from the data source into a pandas DataFrame using .get_dataframe().to_pandas(). \n",
    "This is typically helpful when developing and testing data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ab779-be8f-49ec-b1cb-49d2f9a76db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "\n",
    "transactions.get_dataframe().to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2863323-0a84-4e8e-988e-433af9906c52",
   "metadata": {},
   "source": [
    "### Review Questions:\n",
    "* What is a Tecton [data source](https://docs.tecton.ai/docs/defining-features/data-sources)?  \n",
    "* Where is the Tecton documentation found?  \n",
    "* Where is the documentation of how to read from various sources of data in Tecton?  \n",
    "* (Advanced) How can you read from data in a format or location not directly supported by a Tecton FileConfig?  \n",
    "* (Advanced) What are the 4 types of Tecton DataSources?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9b4cd8-318b-47f0-85e5-4439fda665b2",
   "metadata": {},
   "source": [
    "## B) Create the entity and feature view logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eace994-772d-4e83-ba60-2d1747c962da",
   "metadata": {},
   "source": [
    "Now that we have our data source created, we can now create our feature view, which contains both the transformation logic we want to run on our data source as well as orchestration parameters.\n",
    "\n",
    "An **entity** tells Tecton what the join key is, in this case the `USER_ID` column. Tecton will check to make sure this column(s) exist after running our transformation logic, so we need to make sure we return a `USER_ID` column from our feature view\n",
    "\n",
    "There are two ways to reference an entity. We will be manually creating a new entity, but you can also pull already created Entities from the Tecton cluster\n",
    "\n",
    "```python\n",
    "import tecton\n",
    "ws = tecton.get_workspace('prod')\n",
    "user = ws.get_entity('fraud_user')\n",
    "```\n",
    "\n",
    "#### Creating a local Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a81de-38bd-49ac-9bea-41541816c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton import Entity\n",
    "\n",
    "user = Entity(\n",
    "    name='fraud_user',\n",
    "    join_keys=['USER_ID']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9510293e-f34a-475f-bb9b-de9c837b9aec",
   "metadata": {},
   "source": [
    "Feature Views take in data sources as inputs, or in some cases other Feature Views, and define a transformation to compute one or more features. Feature Views also provide Tecton with additional information such as metadata and orchestration, serving, and monitoring configurations. There are three types of Feature Views, each designed to support a common data flow pattern **(Batch, Streaming, On-Demand)**.\n",
    "\n",
    "**Feature Views** are defined by adding a decorator (e.g **@batch_feature_view**) on top of a Python function.  We specify the:\n",
    "* Data source,\n",
    "* Entity,\n",
    "* Transformation (defined inline or reused from other feature views),\n",
    "* Configuration parameters controlling where and how frequently Tecton materializes the data into the feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cda41e-364c-46d8-8743-6ed04e593d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton import Entity, BatchSource, FileConfig, batch_feature_view, Aggregation\n",
    "from tecton.types import Field, String, Timestamp, Float64\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "@batch_feature_view(\n",
    "    description=\"User transaction metrics over 1, 3 and 7 days\",\n",
    "    sources=[transactions],\n",
    "    entities=[user],\n",
    "    mode=\"pandas\",\n",
    "    aggregation_interval=timedelta(days=1),\n",
    "    aggregations=[\n",
    "        Aggregation(function=\"mean\", column=\"AMT\", time_window=timedelta(days=1)),\n",
    "        Aggregation(function=\"mean\", column=\"AMT\", time_window=timedelta(days=3)),\n",
    "        Aggregation(function=\"mean\", column=\"AMT\", time_window=timedelta(days=7)),\n",
    "        Aggregation(function=\"count\", column=\"AMT\", time_window=timedelta(days=1)),\n",
    "        Aggregation(function=\"count\", column=\"AMT\", time_window=timedelta(days=3)),\n",
    "        Aggregation(function=\"count\", column=\"AMT\", time_window=timedelta(days=7)),\n",
    "    ],\n",
    "    schema=[Field(\"USER_ID\", String), Field(\"TIMESTAMP\", Timestamp), Field(\"AMT\", Float64)],\n",
    ")\n",
    "def user_transaction_metrics(transactions):\n",
    "    return transactions[[\"USER_ID\", \"TIMESTAMP\", \"AMT\"]]\n",
    "\n",
    "\n",
    "# After we define local objects, we use `.validate()` to check the correctness of the definition and make it ready to query\n",
    "user_transaction_metrics.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f4984f-bd75-48c1-a32b-9e25ddc6d4ee",
   "metadata": {},
   "source": [
    "### Test the feature view\n",
    "\n",
    "Now that our feature View is defined, we can test it locally, we can do that in two ways:\n",
    "- Compute feature values for a given range, defined by a ```start_time``` and ```end_time```\n",
    "- Compute feature values based on a set of training events (join keys + timestamps) also called a spine dataframe\n",
    "\n",
    "Here, we'll simply compute feature values for a 6 months range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8893a525-f873-47e0-8403-bd6d980384a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(2023,1,1)\n",
    "end = datetime(2023,6,1)\n",
    "\n",
    "tdf = user_transaction_metrics.get_historical_features(start_time=start, end_time=end)\n",
    "type(tdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb2366e-2255-4510-bf97-61b8a6ba33de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can convert this Tecton DataFrame into a Pandas dataframe (or Spark dataframe)\n",
    "tdf.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05fb802-0002-4318-b6ae-59f41f6a1324",
   "metadata": {},
   "source": [
    "### Review Questions:\n",
    "* What is a Tecton [entity](https://docs.tecton.ai/docs/defining-features/entities)?  \n",
    "* What is a Tecton [feature view](https://docs.tecton.ai/docs/defining-features/feature-views)?  \n",
    "* What is a Tecton [dataframe](https://docs.tecton.ai/docs/sdk-reference/interacting-with-the-feature-store/tecton.TectonDataFrame)?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d38f1-226b-41ae-8ec7-85e2d0a69676",
   "metadata": {},
   "source": [
    "## C) Create a Feature Service\n",
    "\n",
    "A **Feature Service** represents the features that are needed by a model (or group of models). A feature service traditionally sources features from multiple feature views.\n",
    "\n",
    "You can create and test a feature service by:\n",
    "\n",
    "1. Adding a feature view to an existing feature service\n",
    "2. Create a net new feature service\n",
    "\n",
    "Here we are locally creating a new version of the feature_service and adding a new feature to it.\n",
    "```\n",
    "from tecton import FeatureService\n",
    "\n",
    "existing_feature_service = ws.get_feature_service(\"fraud_service\")\n",
    "feature_list = existing_feature_service.get_features_list()\n",
    "\n",
    "fraud_service_v2 = FeatureService(\n",
    "    name=\"fraud_service:v2\",\n",
    "    features=feature_list+user_transaction_metrics # add the new feature to the features list\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7784ae-1705-4db4-99bd-9e04c981688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton import FeatureService\n",
    "\n",
    "\n",
    "fraud_detection_feature_service = FeatureService(\n",
    "    name=\"fraud_detection_feature_service\", features=[user_transaction_metrics]\n",
    ")\n",
    "\n",
    "fraud_detection_feature_service.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdec092-cd0f-4fee-96d4-33ca19a0a31a",
   "metadata": {},
   "source": [
    "### Generate training data from our service\n",
    "We'll build our training dataset from labeled historical transactions and try to predict the \"is_fraud\" column for a given transaction.\n",
    "\n",
    "Let's load up some raw training events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db0e5fb-1892-4ae0-97ed-d7c8608d9b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_snowflake(query):\n",
    "    df = conn.cursor().execute(query).fetch_pandas_all()\n",
    "    return df\n",
    "\n",
    "training_events = query_snowflake(\"\"\"\n",
    "    select \n",
    "        USER_ID,\n",
    "        TIMESTAMP,\n",
    "        AMT,\n",
    "        IS_FRAUD\n",
    "    from\n",
    "        TRANSACTIONS\n",
    "\"\"\")\n",
    "\n",
    "display(training_events.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29db08df-3b52-43ec-8376-358d9e691f98",
   "metadata": {},
   "source": [
    "Once these labelled training events have been loaded into a pandas DataFrame, we want to enrich them with features from Tecton in a point in time accurate way.\n",
    "We can pass our ```training_events``` as an argument of our ```.get_historical_features()``` function. For each entity ID and timestamp in our training events DataFrame, Tecton will compute/read from all Feature Views in our Feature Service, join values based on timestamp and return an enriched dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146d8f7-56d5-4a3e-840d-a33e684e471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = fraud_detection_feature_service.get_historical_features(training_events).to_pandas().fillna(0)\n",
    "display(training_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2180eeed-6589-4ab8-9173-9e456e4ea9f6",
   "metadata": {},
   "source": [
    "## D) Train a Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dadf70-5947-4abf-858c-a11d47a0f005",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8cbd95-4f2d-44cb-aa32-56442e44fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "df = training_data.drop([\"USER_ID\", \"TIMESTAMP\", \"AMT\"], axis=1)\n",
    "\n",
    "X = df.drop(\"IS_FRAUD\", axis=1)\n",
    "y = df[\"IS_FRAUD\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "num_cols = X_train.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "num_pipe = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "\n",
    "cat_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"N/A\"), OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    ")\n",
    "\n",
    "full_pipe = ColumnTransformer([(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)])\n",
    "\n",
    "model = make_pipeline(full_pipe, LogisticRegression(max_iter=1000, random_state=42))\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_predict = model.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_predict, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e763e5e-7138-4721-bc70-7c00c2127b63",
   "metadata": {},
   "source": [
    "# 2. Deploy features to Tecton\n",
    "\n",
    "Tecton objects get registered via a declarative workflow. Features are defined as code in a repo and applied to a workspace in a Tecton account using the Tecton CLI. This approach enables productionisation best practices such as \"features as code,\" CI/CD, and unit testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f26a9da-bcb7-4034-b5f1-a828be2e3265",
   "metadata": {},
   "source": [
    "### 1. Create Tecton repository\n",
    "\n",
    "Let's switch over from our notebook to a terminal and create a new Tecton Feature Repository. For now we will put all our definitions in a single file.\n",
    "\n",
    "âœ… Run these commands to create a new Tecton repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3714720a-0fee-440b-8ea0-0322a00da175",
   "metadata": {},
   "source": [
    "```\n",
    "mkdir tecton-feature-repo\n",
    "cd tecton-feature-repo\n",
    "touch features.py\n",
    "tecton init\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceca2ad6-e7c4-4c8f-84bb-719795b1bc12",
   "metadata": {},
   "source": [
    "### 2. Copy definitions from the notebook and enable materialization\n",
    "âœ… Now copy & paste the definition of the Tecton objects you created in your notebook to ```features.py``` (copied below).\n",
    "\n",
    "On our Feature View we've added four parameters to enable backfilling and ongoing materialization to the online and offline Feature Store:\n",
    "\n",
    "```\n",
    "online=True,\n",
    "offline=True,\n",
    "feature_start_time=datetime(2023,1,1),\n",
    "batch_schedule=timedelta(days=1)\n",
    "```\n",
    "When we apply our changes to a Live Workspace, Tecton will automatically kick off jobs to backfill feature data from ```feature_start_time```. Frontfill jobs will then run on the defined ```batch_schedule```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9672a1a-06f8-4fb1-8456-f355b8510e43",
   "metadata": {},
   "source": [
    "```python\n",
    "from tecton import Entity, BatchSource,SnowflakeConfig, batch_feature_view, Aggregation, DeltaConfig, FeatureService\n",
    "from tecton.types import Field, String, Timestamp, Float64, Int64\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "snowflake_config = SnowflakeConfig(\n",
    "    url=\"https://tectonpartner.snowflakecomputing.com/\",\n",
    "    database=\"NAB_MFT_DB\",\n",
    "    schema=\"PUBLIC\",\n",
    "    warehouse=\"NAB_WH\",\n",
    "    table=\"TRANSACTIONS\",\n",
    "    timestamp_field=\"TIMESTAMP\"\n",
    ")\n",
    "\n",
    "transactions = BatchSource(\n",
    "    name='transactions',\n",
    "    batch_config=snowflake_config,\n",
    ")\n",
    "\n",
    "# An entity defines the concept we are modeling features for\n",
    "# The join keys will be used to aggregate, join, and retrieve features\n",
    "user = Entity(name=\"user\", join_keys=[\"USER_ID\"])\n",
    "\n",
    "# We use SQL to transform the raw data and Tecton aggregations to efficiently and accurately compute metrics across raw events\n",
    "# Feature View decorators contain a wide range of parameters for materializing, cataloging, and monitoring features\n",
    "@batch_feature_view(\n",
    "    description=\"User transaction metrics over 1, 3 and 7 days\",\n",
    "    sources=[transactions],\n",
    "    entities=[user],\n",
    "    mode=\"pandas\",\n",
    "    aggregation_interval=timedelta(days=1),\n",
    "    aggregations=[\n",
    "        Aggregation(function=\"mean\", column=\"AMT\", time_window=timedelta(days=1)),\n",
    "        Aggregation(function=\"mean\", column=\"AMT\", time_window=timedelta(days=3)),\n",
    "        Aggregation(function=\"mean\", column=\"AMT\", time_window=timedelta(days=7)),\n",
    "        Aggregation(function=\"count\", column=\"AMT\", time_window=timedelta(days=1)),\n",
    "        Aggregation(function=\"count\", column=\"AMT\", time_window=timedelta(days=3)),\n",
    "        Aggregation(function=\"count\", column=\"AMT\", time_window=timedelta(days=7)),\n",
    "    ],\n",
    "    schema=[Field(\"USER_ID\", String), Field(\"TIMESTAMP\", Timestamp), Field(\"AMT\", Float64)],\n",
    "    offline_store=DeltaConfig(),\n",
    "    online=True,\n",
    "    offline=True,\n",
    "    feature_start_time=datetime(2023, 1, 1),\n",
    "    batch_schedule=timedelta(days=1)\n",
    ")\n",
    "def user_transaction_metrics(transactions):\n",
    "    return transactions[[\"USER_ID\", \"TIMESTAMP\", \"AMT\"]]\n",
    "\n",
    "\n",
    "fraud_detection_feature_service = FeatureService(\n",
    "    name=\"fraud_detection_feature_service\", features=[user_transaction_metrics]\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7878b5-74b3-4790-8274-3e504125fc13",
   "metadata": {},
   "source": [
    "### 3. Apply your changes to a new workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb785c6-8cdf-4c2b-a2af-9a9b11d91765",
   "metadata": {},
   "source": [
    "Our last step is to login to your organization's Tecton account and apply our repo to a workspace!\n",
    "\n",
    "âœ… Run the following commands in your terminal to create a workspace and apply your changes:\n",
    "\n",
    "```\n",
    "tecton login https://demo-nebula.tecton.ai/\n",
    "tecton workspace create [your-name]-quickstart --live\n",
    "tecton apply\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad778096-01c9-4e72-a691-2f880bdb0e88",
   "metadata": {},
   "source": [
    "The output of ```tecton apply``` will look like this:\n",
    "```\n",
    "Using workspace \"[your-name]-quickstart\" on cluster https://app.tecton.ai\n",
    "âœ… Imported 1 Python module from the feature repository\n",
    "âœ… Imported 1 Python module from the feature repository\n",
    "âš ï¸  Running Tests: No tests found.\n",
    "âœ… Collecting local feature declarations\n",
    "âœ… Performing server-side feature validation: Initializing.\n",
    " â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“ Plan Start â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“\n",
    "\n",
    "  + Create Batch Data Source\n",
    "    name:           transactions\n",
    "\n",
    "  + Create Entity\n",
    "    name:           user\n",
    "\n",
    "  + Create Transformation\n",
    "    name:           user_transaction_metrics\n",
    "    description:    Trailing average transaction amount over 1, 3 and 7 days\n",
    "\n",
    "  + Create Batch Feature View\n",
    "    name:           user_transaction_metrics\n",
    "    description:    Trailing average transaction amount over 1, 3 and 7 days\n",
    "    materialization: 11 backfills, 1 recurring batch job\n",
    "    > backfill:     10 Backfill jobs 2021-12-25 00:00:00 UTC to 2023-08-16 00:00:00 UTC writing to the Offline Store\n",
    "                    1 Backfill job 2023-08-16 00:00:00 UTC to 2023-08-23 00:00:00 UTC writing to both the Online and Offline Store\n",
    "    > incremental:  1 Recurring Batch job scheduled every 1 day writing to both the Online and Offline Store\n",
    "\n",
    "  + Create Feature Service\n",
    "    name:           fraud_detection_feature_service\n",
    "\n",
    " â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘ Plan End â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘\n",
    " Generated plan ID is 8d01ad78e3194a5dbd3f934f04d71564\n",
    " View your plan in the Web UI: https://app.tecton.ai/app/[your-name]-quickstart/plan-summary/8d01ad78e3194a5dbd3f934f04d71564\n",
    " âš ï¸  Objects in plan contain warnings.\n",
    "\n",
    "Note: Updates to Feature Services may take up to 60 seconds to be propagated to the real-time feature-serving endpoint.\n",
    "Note: This workspace ([your-name]-quickstart) is a \"Live\" workspace. Applying this plan may result in new materialization jobs which will incur costs. Carefully examine the plan output before applying changes.\n",
    "Are you sure you want to apply this plan to: \"[your-name]-quickstart\"? [y/N]> y\n",
    "ðŸŽ‰ all done!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298bbec7-4de9-4fb1-8a55-dab08c44856f",
   "metadata": {},
   "source": [
    "# 3. Deploy Model and read features from Tecton in Real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a6d23f-c5e6-4f4f-81d0-66fb79a41fce",
   "metadata": {},
   "source": [
    "### A) Retrieve features at low-latency\n",
    "\n",
    "Our real-time Fraud detection model needs access to feature data within a very small latency budget, typically the end-to-end budget for feature retrieval + model prediction is around 100ms.\n",
    "\n",
    "Once features are deployed to Tecton, they can be consumed in real-time in a variety of ways: \n",
    "- [REST API](https://docs.tecton.ai/docs/reading-feature-data/reading-feature-data-for-inference/reading-online-features-for-inference-using-the-http-api)\n",
    "- [Python Client](https://docs.tecton.ai/docs/reading-feature-data/reading-feature-data-for-inference/reading-online-features-for-inference-using-the-python-client)\n",
    "- [Java Client](https://docs.tecton.ai/docs/reading-feature-data/reading-feature-data-for-inference/reading-online-features-for-inference-using-the-java-client)\n",
    "\n",
    "Here, let's use Tecton's REST API to retrieve features at low latency.\n",
    "\n",
    "To do this, you will first need to create a new Service Account and give it access to read features from your workspace. This Service Account will be used to authenticate your calls to Tecton.\n",
    "\n",
    "Follow these commands in your terminal:\n",
    "\n",
    "```\n",
    "tecton service-account create --name \"[your-name]-quickstart\" --description \"Quickstart service account\"\n",
    "tecton access-control assign-role -r consumer -w [your-name]-quickstart -s [service account id from last command]\n",
    "```\n",
    "\n",
    "You will use the API key from the first command in the cell below where we define a function to retrieve online feature data for a given user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ef997-9fef-4256-96e4-6c1775818589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "\n",
    "def get_online_feature_data(user_id):\n",
    "    TECTON_API_KEY = \"[your-api-key]\"\n",
    "    WORKSPACE_NAME = \"[your-workspace-name]\"\n",
    "\n",
    "    headers = {\"Authorization\": \"Tecton-key \" + TECTON_API_KEY}\n",
    "\n",
    "    request_data = f\"\"\"{{\n",
    "        \"params\": {{\n",
    "            \"feature_service_name\": \"fraud_detection_feature_service\",\n",
    "            \"join_key_map\": {{\"USER_ID\": \"{user_id}\"}},\n",
    "            \"metadata_options\": {{\"include_names\": true}},\n",
    "            \"workspace_name\": \"{WORKSPACE_NAME}\"\n",
    "        }}\n",
    "    }}\"\"\"\n",
    "\n",
    "    online_feature_data = requests.request(\n",
    "        method=\"POST\",\n",
    "        headers=headers,\n",
    "        url=f\"https://demo-nebula.tecton.ai/api/v1/feature-service/get-features\",\n",
    "        data=request_data,\n",
    "    )\n",
    "\n",
    "    online_feature_data_json = json.loads(online_feature_data.text)\n",
    "\n",
    "    return online_feature_data_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d706a9b-e923-410b-ac1d-92bb2df9b8e9",
   "metadata": {},
   "source": [
    "Now we can use our function to retrieve features at low latency!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd4643-2a25-4c45-8bd7-4e80f9ecb094",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"user_502567604689\"\n",
    "\n",
    "feature_data = get_online_feature_data(user_id)\n",
    "\n",
    "if \"result\" not in feature_data:\n",
    "    print(\"Feature data is not materialized\")\n",
    "else:\n",
    "    print(feature_data[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158e65e1-0624-4e3c-8ca4-3ba8bc26a0df",
   "metadata": {},
   "source": [
    "### B) Make model predictions\n",
    "\n",
    "Now that we can fetch feature data online, let's create a function that takes a feature vector and runs model inference to get a fraud prediction.\n",
    "\n",
    "ðŸ’¡**INFO**\n",
    "Typically you'd instead use a model serving API that is hosting your model. Here we run inference directly in our notebook for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5c47d9-5898-4fa0-8d50-0f279ee7d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_prediction_from_model(feature_data):\n",
    "    columns = [f[\"name\"].replace(\".\", \"__\") for f in feature_data[\"metadata\"][\"features\"]]\n",
    "    data = [feature_data[\"result\"][\"features\"]]\n",
    "\n",
    "    features = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    return model.predict(features)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35bf83a-e0c1-42be-a540-d4eeaffeecc1",
   "metadata": {},
   "source": [
    "**Let's put it all together and run inference!**\n",
    "\n",
    "We can fetch our online features from Tecton, call our inference function, and get a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b39f3dd-9d6f-49bb-8e10-0798f2c8e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"user_502567604689\"\n",
    "\n",
    "online_feature_data = get_online_feature_data(user_id)\n",
    "prediction = get_prediction_from_model(online_feature_data)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9838e7f-88cd-4c18-acfe-15b109c72a57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
